# bandit_hw.py
# Minimal MDP (Bandit) + Figure 2.2 reproduction + Gradient Bandit (4 settings)

import numpy as np
import matplotlib.pyplot as plt

# ---------------------------
# Minimal MDP Framework (Bandit)
# ---------------------------

class Env:
    def reset(self):
        raise NotImplementedError
    def step(self, action):
        raise NotImplementedError

class Bandit(Env):
    """
    Stationary k-armed bandit.
    True action values q_true ~ N(0,1); reward ~ N(q_true[a], 1).
    """
    def __init__(self, k=10):
        self.k = k
        self.reset()

    def reset(self):
        self.q_true = np.random.normal(0.0, 1.0, size=self.k)
        return None

    def step(self, action: int):
        reward = np.random.normal(self.q_true[action], 1.0)
        return None, reward, False, {}  # (next_state, reward, done, info)

# ---------------------------
# Agents
# ---------------------------

class Agent:
    def select_action(self):
        raise NotImplementedError
    def update(self, action, reward, next_state=None):
        raise NotImplementedError

class EpsilonGreedyAgent(Agent):
    """
    ε-greedy with sample-average update (as in Sutton & Barto Figure 2.2).
    """
    def __init__(self, k=10, epsilon=0.1, q_init=0.0):
        self.k = k
        self.epsilon = epsilon
        self.q_est = np.full(k, q_init, dtype=float)
        self.action_count = np.zeros(k, dtype=int)

    def select_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.k)
        return int(np.argmax(self.q_est))

    def update(self, action, reward, next_state=None):
        self.action_count[action] += 1
        step = 1.0 / self.action_count[action]  # sample-average
        self.q_est[action] += step * (reward - self.q_est[action])

class GradientBanditAgent(Agent):
    """
    Gradient bandit with softmax over preferences H.
    Four settings will be tested:
      - no baseline, α=0.1
      - no baseline, α=0.4
      - with baseline, α=0.1
      - with baseline, α=0.4
    """
    def __init__(self, k=10, alpha=0.1, baseline=True):
        self.k = k
        self.alpha = alpha
        self.baseline = baseline
        self.H = np.zeros(k, dtype=float)
        self.pi = np.ones(k, dtype=float) / k
        self.t = 0
        self.avg_reward = 0.0

    def _update_policy(self):
        m = np.max(self.H)
        expH = np.exp(self.H - m)
        self.pi = expH / np.sum(expH)

    def select_action(self):
        self._update_policy()
        return int(np.random.choice(self.k, p=self.pi))

    def update(self, action, reward, next_state=None):
        self.t += 1
        baseline = 0.0
        if self.baseline:
            self.avg_reward += (reward - self.avg_reward) / self.t
            baseline = self.avg_reward
        self._update_policy()
        onehot = np.zeros(self.k)
        onehot[action] = 1.0
        self.H += self.alpha * (reward - baseline) * (onehot - self.pi)

# ---------------------------
# Experiment
# ---------------------------

def simulate(agent_ctor, runs=2000, steps=1000, env_ctor=Bandit, **agent_kwargs):
    """
    Returns:
        mean_rewards: (steps,)
        mean_optimal: (steps,) fraction of optimal action chosen
    """
    rewards = np.zeros((runs, steps), dtype=float)
    optimal = np.zeros((runs, steps), dtype=float)

    for r in range(runs):
        env = env_ctor()
        env.reset()
        agent = agent_ctor(**agent_kwargs)
        optimal_action = int(np.argmax(env.q_true))

        for t in range(steps):
            a = agent.select_action()
            _, rew, _, _ = env.step(a)
            agent.update(a, rew, None)
            rewards[r, t] = rew
            optimal[r, t] = 1.0 if a == optimal_action else 0.0

    return rewards.mean(axis=0), optimal.mean(axis=0)

def main():
    # Fixed configuration per assignment
    runs = 2000
    steps = 1000
    seed = 0
    np.random.seed(seed)

    # Figure 2.2 reproduction: ε = 0, 0.01, 0.1 (sample-average updates)
    _, opt_eps0   = simulate(EpsilonGreedyAgent, runs=runs, steps=steps, epsilon=0.0)
    _, opt_eps001 = simulate(EpsilonGreedyAgent, runs=runs, steps=steps, epsilon=0.01)
    _, opt_eps01  = simulate(EpsilonGreedyAgent, runs=runs, steps=steps, epsilon=0.1)

    # Gradient bandit: four required settings
    _, opt_grad_nb_a01 = simulate(GradientBanditAgent, runs=runs, steps=steps, alpha=0.1, baseline=False)
    _, opt_grad_nb_a04 = simulate(GradientBanditAgent, runs=runs, steps=steps, alpha=0.4, baseline=False)
    _, opt_grad_b_a01  = simulate(GradientBanditAgent, runs=runs, steps=steps, alpha=0.1, baseline=True)
    _, opt_grad_b_a04  = simulate(GradientBanditAgent, runs=runs, steps=steps, alpha=0.4, baseline=True)

    # Plot: Figure 2.2 (three ε-greedy curves) + four gradient curves
    plt.figure(figsize=(12, 8))
    plt.plot(opt_eps0,   label="ε-greedy ε=0 (sample-average)")
    plt.plot(opt_eps001, label="ε-greedy ε=0.01 (sample-average)")
    plt.plot(opt_eps01,  label="ε-greedy ε=0.1 (sample-average)")
    plt.plot(opt_grad_nb_a01, label="Gradient α=0.1 (no baseline)")
    plt.plot(opt_grad_nb_a04, label="Gradient α=0.4 (no baseline)")
    plt.plot(opt_grad_b_a01,  label="Gradient α=0.1 (with baseline)")
    plt.plot(opt_grad_b_a04,  label="Gradient α=0.4 (with baseline)")
    plt.xlabel("Steps")
    plt.ylabel("% Optimal Action")
    plt.title("Figure 2.2 Reproduction + Gradient Bandit (4 settings)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig("figure_2_2_with_gradient.png", dpi=160)
    plt.show()

if __name__ == "__main__":
    main()
